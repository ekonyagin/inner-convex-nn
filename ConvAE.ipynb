{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "%matplotlib inline\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torch_data\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poslayers.poslayers import Dense, PosDense, PosConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VAE.src.vanila_vae import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CelebA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple ConvAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "from   torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from   torchvision import datasets, transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebA():\n",
    "    def __init__(self, root_path, shape=(128, 128), dataset_size=202599, train_part=0.8):\n",
    "        self.root_path = root_path\n",
    "        self.shape = shape\n",
    "        self.dataset_size = dataset_size\n",
    "        self.train_size = round(train_part * dataset_size)\n",
    "        self.totensor = transforms.ToTensor()\n",
    "    \n",
    "    def __get_item__(self, index):\n",
    "        name = \"{:06d}.jpg\".format(index + 1)\n",
    "        img = Image(f'{self.root_path}/{name}').resize(self.shape)\n",
    "        return self.totensor(img)\n",
    "    \n",
    "    def load_train_batch(self, batch_first_idx, batch_size):\n",
    "        indexes = batch_first_idx + 1 + np.arange(min(batch_size,\n",
    "                                                      self.train_size - batch_first_idx))\n",
    "        names = [\"{:06d}.jpg\".format(ind) for ind in indexes]\n",
    "        \n",
    "        batch = []\n",
    "        for name in names:\n",
    "            img = Image.open(f'{self.root_path}/{name}').resize(self.shape)\n",
    "            batch.append(self.totensor(img))\n",
    "        return torch.stack(batch, dim=0)\n",
    "    \n",
    "    def load_test_batch(self, batch_idx, batch_size):\n",
    "        indexes = self.train_size + batch_first_idx + 1 + np.arange(min(batch_size,\n",
    "                                                                        self.dataset_size - batch_first_idx - self.train_size))\n",
    "        names = [\"{:06d}.jpg\".format(ind) for ind in indexes]\n",
    "\n",
    "        batch = []\n",
    "        for name in names:\n",
    "            img = Image.open(f'{root_path}/{name}').resize(self.shape)\n",
    "            batch.append(self.totensor(img))\n",
    "        return torch.stack(batch, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, code_size):\n",
    "        super().__init__()\n",
    "        self.code_size = code_size\n",
    "        \n",
    "        # Encoder specification\n",
    "        self.enc_cnn_1 = nn.Conv2d(INPUT_CHANNELS, 5, kernel_size=5)\n",
    "        self.enc_cnn_2 = nn.Conv2d(5, 10, kernel_size=5)\n",
    "        self.enc_linear_1 = nn.Linear(10 * 7 * 7, 800)\n",
    "        self.enc_linear_2 = nn.Linear(800, self.code_size)\n",
    "        \n",
    "        # Decoder specification\n",
    "        self.dec_linear_1 = nn.Linear(self.code_size, 4000)\n",
    "        self.dec_linear_2 = nn.Linear(4000, IMAGE_SIZE * INPUT_CHANNELS)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        code = self.encode(images)\n",
    "        out = self.decode(code)\n",
    "        return out, code\n",
    "    \n",
    "    def encode(self, images):\n",
    "        code = self.enc_cnn_1(images)\n",
    "        code = F.selu(F.max_pool2d(code, 2))\n",
    "        \n",
    "        code = self.enc_cnn_2(code)\n",
    "        code = F.selu(F.max_pool2d(code, 2))\n",
    "        \n",
    "        code = code.view([images.size(0), -1])\n",
    "        code = F.selu(self.enc_linear_1(code))\n",
    "        code = self.enc_linear_2(code)\n",
    "        return code\n",
    "    \n",
    "    def decode(self, code):\n",
    "        out = F.selu(self.dec_linear_1(code))\n",
    "        out = F.sigmoid(self.dec_linear_2(out))\n",
    "        out = out.view([code.size(0), INPUT_CHANNELS, IMAGE_WIDTH, IMAGE_HEIGHT])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 40 * 40\n",
    "IMAGE_WIDTH = IMAGE_HEIGHT = 40\n",
    "INPUT_CHANNELS = 3\n",
    "\n",
    "# Hyperparameters\n",
    "code_size = 500\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "lr = 0.002\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "\n",
    "net = AutoEncoder(code_size=code_size)\n",
    "celeba = CelebA('./celeba/img_align_celeba', shape=(IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(epochs, net, criterion, optimizer, ds, batch_size=128, \n",
    "             scheduler=None, verbose=True, save_dir=None, device=0):\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        net.train()\n",
    "        for batch_idx in tqdm(range(0, ds.train_size, batch_size)):\n",
    "            batch = ds.load_train_batch(batch_idx, batch_size)\n",
    "            out, code = net(Variable(batch))\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(out, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        net.eval()\n",
    "        for batch_idx in range(0, ds.ds_size - ds.train_size, batch_size):\n",
    "            batch = ds.load_test_batch(batch_idx, batch_size)\n",
    "            out, code = net(Variable(batch))\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(out, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "         \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        freq = 1\n",
    "        if verbose and epoch%freq==0:\n",
    "            print('Epoch {}/{} || Loss:  Train {:.4f} | Validation {:.4f}'.format(epoch, epochs, np.mean(loss), np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a98bf2665f747e0b0dd6ab45edce6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2533), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ae(1, net, loss_fn, optimizer, celeba, batch_size=64, \n",
    "         scheduler=None, verbose=True, save_dir=None, device=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
