{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from torch.nn import Conv2d as Conv2D\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.nn import Upsample\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungCTDataset(Dataset):\n",
    "    \"\"\"LungCT dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, size=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        self.image_frame = pd.read_csv(csv_file, skiprows=1)\n",
    "        self.root_dir = root_dir\n",
    "        self.size=512\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        size = self.size\n",
    "        img_name = os.path.join(self.root_dir, self.image_frame.iloc[idx, 0])\n",
    "        mask_name = os.path.join(self.root_dir, self.image_frame.iloc[idx, 1])\n",
    "        \n",
    "        image = cv2.imread(img_name, 0)/255.\n",
    "        image.resize(size, size)\n",
    "        image = image.reshape((1, size, size))\n",
    "        mask = cv2.imread(mask_name, 0)/255.\n",
    "        mask.resize(size, size)\n",
    "        mask = mask.reshape((1, size, size))\n",
    "        sample = {'image': image, 'mask': mask}\n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out):\n",
    "        super(Up, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv2D(channel_in, channel_out, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(channel_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # Input size - Batch_Size X Channel X Height of Activation Map  X Width of Activation Map\n",
    "        # Upsample using bilinear mode and scale it to twice its size\n",
    "        x1 = self.upsample(x1)\n",
    "        # in 4D array - matching the last two in case of 5D it will take \n",
    "        # last three dimensions\n",
    "        difference_in_X = x1.size()[2] - x2.size()[2]\n",
    "        difference_in_Y = x1.size()[3] - x2.size()[3]\n",
    "        # Padding it with the required value\n",
    "        x2 = F.pad(x2, (difference_in_X // 2, int(difference_in_X / 2),\n",
    "                        difference_in_Y // 2, int(difference_in_Y / 2)))\n",
    "        # concat on channel axis\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        # Use convolution\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out):\n",
    "        super(Down, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv2D(channel_in, channel_out, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(channel_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input size - Batch_Size X Channel X Height of Activation Map  X Width of Activation Map\n",
    "        # Downsample First\n",
    "        x = F.max_pool2d(x,2)\n",
    "        # Use convolution\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, channel_in, classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.input_conv = self.conv = nn.Sequential(\n",
    "            Conv2D(channel_in, 8, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.down1 = Down(8, 16)\n",
    "        self.down2 = Down(16, 32)\n",
    "        self.down3 = Down(32, 32)\n",
    "        self.up1 = Up(64, 16)\n",
    "        self.up2 = Up(32, 8)\n",
    "        self.up3 = Up(16, 4)\n",
    "        self.output_conv = nn.Conv2d(4, classes, kernel_size = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.input_conv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x = self.up1(x4, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up3(x, x1)\n",
    "        output = self.output_conv(x)\n",
    "        return F.sigmoid(output)\n",
    "    \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_uniform(m.weight, gain=np.sqrt(2.0))\n",
    "        init.constant(m.bias, 0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Dataset\n",
    "IMAGE_DIR = \"Dataset/2d_images/\"\n",
    "MASK_DIR = \"Dataset/2d_masks/\"\n",
    "with open('Dataset/lungs.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"filename\", \"mask\"])\n",
    "    for p in os.listdir(IMAGE_DIR):\n",
    "        image_path = os.path.join(IMAGE_DIR, p)\n",
    "        mask_path = os.path.join(MASK_DIR, p)\n",
    "        writer.writerow([image_path, mask_path])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Dataset/lungs.csv\")\n",
    "data = data.iloc[np.random.permutation(len(data))]\n",
    "p = int(len(data)*0.7)\n",
    "train, validation = data[:p], data[p:]\n",
    "train.to_csv(\"Dataset/lungs_train.csv\", index=False)\n",
    "validation.to_csv(\"Dataset/lungs_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lung_ct_train_dataset = LungCTDataset(csv_file='Dataset/lungs_train.csv', root_dir='./')\n",
    "lung_ct_val_dataset = LungCTDataset(csv_file='Dataset/lungs_val.csv', root_dir='./')\n",
    "train_dataloader = DataLoader(lung_ct_train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(lung_ct_val_dataset, batch_size=8, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(y_true, y_pred):\n",
    "    intersection = (y_pred*y_true).sum()\n",
    "    #print(\"int: \", intersection)\n",
    "    union = y_pred.sum() + y_true.sum() - intersection\n",
    "    #print(\"union: \", union)\n",
    "    return intersection/union\n",
    "\n",
    "\n",
    "\n",
    "def Accuracy(target, output, threshold = 0.5):\n",
    "    output = output.cpu().detach().numpy()>0.6\n",
    "    target = target.cpu().numpy()\n",
    "    output = 1.*output.reshape(-1)\n",
    "    target = 1.*target.reshape(-1)\n",
    "    #print(accuracy_score(target, output))\n",
    "    return(accuracy_score(target, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        data, target = (Variable(data[\"image\"]).cuda(), \n",
    "                        Variable(data[\"mask\"]).cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(data.float())\n",
    "        loss = criterion(output.float(), target.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 1 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "                100. * batch_idx / len(train_dataloader), loss.item()), end='\\r')\n",
    "    print()\n",
    "    return loss.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    iou_metrics = 0\n",
    "    acc_metrics = 0\n",
    "    counter = 0\n",
    "    for data in val_dataloader:\n",
    "        counter +=1\n",
    "        data, target = Variable(data[\"image\"], volatile=True).cuda(), Variable(data[\"mask\"]).cuda()\n",
    "        output = model(data.float())\n",
    "        test_loss += criterion(output.float(), target.float()).item() # sum up batch loss\n",
    "        iou = IoU((output>0.6)*1., target*1.).cpu().item()\n",
    "        iou_metrics += iou\n",
    "        acc_metrics += Accuracy(target, output)\n",
    "    test_loss /= counter\n",
    "    \n",
    "    iou_metrics /= counter\n",
    "    acc_metrics /= counter\n",
    "    print(\"Average Loss: \", test_loss)\n",
    "    print(\"Accuracy: \", acc_metrics)\n",
    "    return(test_loss, acc_metrics, iou_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance = UNet(1, 1)\n",
    "model_instance.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 99 % validation accuracy \n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_iou = []\n",
    "val_metrics = []\n",
    "for epoch in range(1,100):\n",
    "    if epoch < 10:\n",
    "        lr = 1e-2\n",
    "    elif epoch < 20:\n",
    "        lr = 1e-3\n",
    "    elif epoch < 30:\n",
    "        lr = 1e-4\n",
    "    else:\n",
    "        lr = 1e-5\n",
    "    optimizer = optim.Adam(model_instance.parameters(), lr)\n",
    "    loss = train(model_instance, epoch, optimizer)\n",
    "    train_losses.append(loss)\n",
    "    test_loss, acc, iou = test(model_instance)\n",
    "    val_losses.append(test_loss)\n",
    "    val_iou.append(iou)\n",
    "    val_metrics.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_non_convex_report = {\"train_losses\": train_losses, \n",
    "                   \"val_losses\": val_losses,\n",
    "                   \"val_iou\": val_iou,\n",
    "                   \"val_metrics\": val_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(val_losses, label=\"val loss\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(val_iou, label=\"val_iou\")\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"IoU\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convex UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up_pos(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out):\n",
    "        super(Up_pos, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv2D(channel_in, channel_out, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(channel_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def positivate(self):\n",
    "        #self.upsample.weight.data = F.relu(self.upsample.weight.data)\n",
    "        self.conv[0].weight.data = F.relu(self.conv[0].weight.data)\n",
    "        \n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        # Input size - Batch_Size X Channel X Height of Activation Map  X Width of Activation Map\n",
    "        # Upsample using bilinear mode and scale it to twice its size\n",
    "        x1 = self.upsample(x1)\n",
    "        # in 4D array - matching the last two in case of 5D it will take \n",
    "        # last three dimensions\n",
    "        difference_in_X = x1.size()[2] - x2.size()[2]\n",
    "        difference_in_Y = x1.size()[3] - x2.size()[3]\n",
    "        # Padding it with the required value\n",
    "        x2 = F.pad(x2, (difference_in_X // 2, int(difference_in_X / 2),\n",
    "                        difference_in_Y // 2, int(difference_in_Y / 2)))\n",
    "        # concat on channel axis\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        # Use convolution\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Down_pos(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out):\n",
    "        super(Down_pos, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv2D(channel_in, channel_out, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(channel_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def positivate(self):\n",
    "        self.conv[0].weight.data = F.relu(self.conv[0].weight.data)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input size - Batch_Size X Channel X Height of Activation Map  X Width of Activation Map\n",
    "        # Downsample First\n",
    "        x = F.max_pool2d(x,2)\n",
    "        # Use convolution\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class UNet_pos(nn.Module):\n",
    "    def __init__(self, channel_in, classes):\n",
    "        super(UNet_pos, self).__init__()\n",
    "        self.input_conv = self.conv = nn.Sequential(\n",
    "            Conv2D(channel_in, 8, kernel_size = 3, padding = 1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.down1 = Down_pos(8, 16)\n",
    "        self.down2 = Down_pos(16, 32)\n",
    "        self.down3 = Down_pos(32, 32)\n",
    "        self.up1 = Up_pos(64, 16)\n",
    "        self.up2 = Up_pos(32, 8)\n",
    "        self.up3 = Up_pos(16, 4)\n",
    "        self.output_conv = nn.Conv2d(4, classes, kernel_size = 1)\n",
    "        \n",
    "    def positivate(self):\n",
    "        #self.input_conv[0].weight.data = F.relu(self.input_conv[0].weight.data)\n",
    "        self.down1.positivate()\n",
    "        self.down2.positivate()\n",
    "        self.down3.positivate()\n",
    "        self.up1.positivate()\n",
    "        self.up2.positivate()\n",
    "        self.up3.positivate()\n",
    "        self.output_conv.weight.data = F.relu(self.output_conv.weight.data)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x1 = self.input_conv(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x = self.up1(x4, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up3(x, x1)\n",
    "        output = self.output_conv(x)\n",
    "        return F.sigmoid(output)\n",
    "    \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_uniform(m.weight, gain=np.sqrt(2.0))\n",
    "        init.constant(m.bias, 0.1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pos(model, epoch, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        data, target = Variable(data[\"image\"]).cuda() , Variable(data[\"mask\"]).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(data.float())\n",
    "        loss = criterion(output.float(), target.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.positivate()\n",
    "        \n",
    "        if batch_idx % 1 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "                100. * batch_idx / len(train_dataloader), loss.item()), end='\\r')\n",
    "    print()\n",
    "    return loss.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = UNet_pos(1,1)\n",
    "pos.cuda()\n",
    "pos.positivate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 99 % validation accuracy \n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_iou = []\n",
    "val_metrics = []\n",
    "for epoch in range(1,100):\n",
    "    if epoch < 10:\n",
    "        lr = 1e-2\n",
    "    elif epoch < 20:\n",
    "        lr = 1e-3\n",
    "    elif epoch < 30:\n",
    "        lr = 1e-4\n",
    "    else:\n",
    "        lr = 1e-5\n",
    "    optimizer = optim.Adam(pos.parameters(), lr)\n",
    "    loss = train_pos(pos, epoch, optimizer)\n",
    "    train_losses.append(loss)\n",
    "    test_loss, acc, iou = test(pos)\n",
    "    val_losses.append(test_loss)\n",
    "    val_iou.append(iou)\n",
    "    val_metrics.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_convex_report = {\"train_losses\": train_losses, \n",
    "                   \"val_losses\": val_losses,\n",
    "                   \"val_iou\": val_iou,\n",
    "                   \"val_metrics\": val_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_report = {\"non_convex\": unet_non_convex_report,\n",
    "                \"convex\": unet_convex_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(train_losses, label=\"train loss\")\n",
    "plt.plot(val_losses, label=\"val loss\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(val_iou, label=\"val_iou\")\n",
    "\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"IoU\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.grid()\n",
    "for model in final_report.keys():\n",
    "    plt.plot(final_report[model]['val_losses'], label=\"UNet_\"+model)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"val_loss\", fontsize=14)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.grid()\n",
    "for model in final_report.keys():\n",
    "    plt.plot(final_report[model]['val_metrics'], label=\"UNet_\"+model)\n",
    "    #plt.ylim((0,1))\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"val_accuracy (pixelwise)\", fontsize=14)\n",
    "plt.savefig(\"loss_acc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,7))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.grid()\n",
    "for model in final_report.keys():\n",
    "    plt.plot(final_report[model]['val_metrics'], label=\"UNet_\"+model)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"val_accuracy (pixelwise)\", fontsize=14)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.grid()\n",
    "for model in final_report.keys():\n",
    "    plt.plot(final_report[model]['val_iou'], label=\"UNet_\"+model)\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"val_IoU_metrics\", fontsize=14)\n",
    "plt.savefig(\"acc_iou.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [i for i in pos.state_dict().keys() if i[-4:]=='ight' or i[-4:]=='bias']\n",
    "\n",
    "Dict = pos.state_dict()\n",
    "summ = 0\n",
    "length = 0\n",
    "for i in keys:\n",
    "    summ +=(Dict[i]==0).sum().item()\n",
    "    print(len(Dict[i].reshape(-1,1)))\n",
    "    length += len(Dict[i].reshape(-1,1))\n",
    "    \n",
    "    #print((Dict[i]==0).sum().item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(pos, input_size=(1,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ/length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [i for i in model_instance.state_dict().keys() if i[-4:]=='ight' or i[-4:]=='bias']\n",
    "\n",
    "Dict = model_instance.state_dict()\n",
    "summ = 0\n",
    "length = 0\n",
    "for i in keys:\n",
    "    summ +=(Dict[i]==0).sum().item()\n",
    "    print(len(Dict[i].reshape(-1,1)))\n",
    "    length += len(Dict[i].reshape(-1,1))\n",
    "    \n",
    "    #print((Dict[i]==0).sum().item())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```  (conv): Sequential(\n",
    "      (0): Conv2d(in_ch, out_ch, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (2): ReLU(inplace=True)\n",
    "    )\n",
    "  )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
