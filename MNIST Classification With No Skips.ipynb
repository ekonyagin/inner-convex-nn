{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "%matplotlib inline\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torch_data\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the dataset.\n",
    "digits, targets = load_digits(return_X_y=True)\n",
    "digits = digits.astype(np.float32) / 255\n",
    "\n",
    "digits_train, digits_test, targets_train, targets_test = train_test_split(digits, targets, random_state=0)\n",
    "\n",
    "train_size = digits_train.shape[0]\n",
    "\n",
    "input_size = 8*8\n",
    "classes_n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTData(torch_data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(MNISTData, self).__init__()\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx],self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(net, val_dset):\n",
    "    test_loader = torch_data.DataLoader(val_dset,batch_size = len(val_dset)) \n",
    "    net.eval()\n",
    "    for X,y in test_loader:\n",
    "        X = X.to(device)\n",
    "        nn_outputs = net(X).detach().numpy().argmax(axis = 1)\n",
    "    return accuracy_score(nn_outputs,y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = MNISTData(digits_train, targets_train) \n",
    "val_dset = MNISTData(digits_test, targets_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-convex Fullyconnected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FCN, self).__init__()\n",
    "        self.fc0 = nn.Linear(64, 40)\n",
    "        self.fc1 = nn.Linear(40, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        h1 = F.relu(self.fc0(input_))\n",
    "        h2 = F.relu(self.fc1(h1))\n",
    "        h3 = self.fc2(h2)\n",
    "        return h3\n",
    "    \n",
    "    def get_sparsities(self):\n",
    "        get_sparsity = lambda layer : (layer.weight.data == 0).sum().item() / (layer.weight.data.shape[0] * layer.weight.data.shape[1])\n",
    "        return {\n",
    "                'fc0': get_sparsity(self.fc0),\n",
    "                'fc1': get_sparsity(self.fc1),\n",
    "                'fc2': get_sparsity(self.fc2),\n",
    "        }\n",
    "    \n",
    "    def l1reg(self):\n",
    "        l1_reg = torch.tensor(0., requires_grad=True)\n",
    "        for name, W in self.named_parameters():\n",
    "            l1_reg = l1_reg +  W.norm(1)\n",
    "        return l1_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net = FCN()  \n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "scheduler = None\n",
    "\n",
    "train_loader = torch_data.DataLoader(train_dset, batch_size=30, shuffle=True) \n",
    "val_loader = torch_data.DataLoader(val_dset, batch_size=100, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fcn(epochs, net, criterion, optimizer, train_loader, val_loader,scheduler=None, verbose=True, save_dir=None, l1alpha=0):\n",
    "    net.to(device)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "        loss = []\n",
    "        for X, y in train_loader:\n",
    "            nn_outputs = net(X)\n",
    "            loss1 = criterion(nn_outputs, y) + l1alpha * net.l1reg()\n",
    "            optimizer.zero_grad()\n",
    "            loss1.backward()\n",
    "            loss.append(loss1.item())\n",
    "            optimizer.step()\n",
    "        net.eval()\n",
    "        val_loss = []\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device)\n",
    "            nn_outputs = net(X)\n",
    "            val_loss1 = criterion(nn_outputs,y)\n",
    "            val_loss.append(val_loss1.item())\n",
    "         \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        freq = max(epochs//20,1)\n",
    "        if verbose and epoch%freq==0:\n",
    "            print('Epoch {}/{} || Loss:  Train {:.4f} | Validation {:.4f}'.format(epoch, epochs, np.mean(loss), np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 || Loss:  Train 1.4474 | Validation 1.4346\n",
      "Epoch 20/200 || Loss:  Train 0.7897 | Validation 0.8063\n",
      "Epoch 30/200 || Loss:  Train 0.5813 | Validation 0.5980\n",
      "Epoch 40/200 || Loss:  Train 0.4897 | Validation 0.5130\n",
      "Epoch 50/200 || Loss:  Train 0.4307 | Validation 0.4520\n",
      "Epoch 60/200 || Loss:  Train 0.3870 | Validation 0.4098\n",
      "Epoch 70/200 || Loss:  Train 0.3351 | Validation 0.3635\n",
      "Epoch 80/200 || Loss:  Train 0.2840 | Validation 0.3125\n",
      "Epoch 90/200 || Loss:  Train 0.2419 | Validation 0.2765\n",
      "Epoch 100/200 || Loss:  Train 0.2135 | Validation 0.2561\n",
      "Epoch 110/200 || Loss:  Train 0.1888 | Validation 0.2441\n",
      "Epoch 120/200 || Loss:  Train 0.1733 | Validation 0.2343\n",
      "Epoch 130/200 || Loss:  Train 0.1578 | Validation 0.2327\n",
      "Epoch 140/200 || Loss:  Train 0.1457 | Validation 0.2249\n",
      "Epoch 150/200 || Loss:  Train 0.1365 | Validation 0.2244\n",
      "Epoch 160/200 || Loss:  Train 0.1255 | Validation 0.2189\n",
      "Epoch 170/200 || Loss:  Train 0.1152 | Validation 0.2217\n",
      "Epoch 180/200 || Loss:  Train 0.1096 | Validation 0.2305\n",
      "Epoch 190/200 || Loss:  Train 0.1012 | Validation 0.2184\n",
      "Epoch 200/200 || Loss:  Train 0.0938 | Validation 0.2203\n"
     ]
    }
   ],
   "source": [
    "train_fcn(200, net, criterion, optimizer, train_loader, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(net, val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc0': 0.0, 'fc1': 0.0, 'fc2': 0.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_sparsities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POSDENSE = CNN WITHOUT ADDITIONAL LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosDense(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PosDense, self).__init__()\n",
    "        self.fcpos0 = nn.Linear(64, 40)\n",
    "        self.fcpos1 = nn.Linear(40, 20)\n",
    "        self.fcpos2 = nn.Linear(20, 10)\n",
    "        self.fc = nn.Linear(10, 10)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        h1 = F.relu(self.fcpos0(input_))\n",
    "        h2 = F.relu(self.fcpos1(h1))\n",
    "        h3 = F.relu(self.fcpos2(h2))\n",
    "        h4 = self.fc(h3)\n",
    "        return h4\n",
    "    \n",
    "    def positivate(self):\n",
    "        self.fcpos0.weight.data = F.relu(self.fcpos0.weight.data)\n",
    "        self.fcpos1.weight.data = F.relu(self.fcpos1.weight.data)\n",
    "        self.fcpos2.weight.data = F.relu(self.fcpos2.weight.data)\n",
    "    \n",
    "    def get_sparsities(self):\n",
    "        get_sparsity = lambda layer : (layer.weight.data == 0).sum().item() / (layer.weight.data.shape[0] * layer.weight.data.shape[1])\n",
    "        return {\n",
    "                'fcpos0': get_sparsity(self.fcpos0),\n",
    "                'fcpos1': get_sparsity(self.fcpos1),\n",
    "                'fcpos2': get_sparsity(self.fcpos2),\n",
    "        }\n",
    "    \n",
    "    def l1reg(self):\n",
    "        l1_reg = torch.tensor(0., requires_grad=True)\n",
    "        for name, W in self.named_parameters():\n",
    "            l1_reg = l1_reg + W.norm(1)\n",
    "        return l1_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net = PosDense()  \n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "scheduler = None\n",
    "\n",
    "train_loader = torch_data.DataLoader(train_dset, batch_size=30, shuffle=True) \n",
    "val_loader = torch_data.DataLoader(val_dset, batch_size=100, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_posdense(epochs, net, criterion, optimizer, train_loader, val_loader,scheduler=None, verbose=True, save_dir=None, l1alpha=0):\n",
    "    net.to(device)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "        loss = []\n",
    "        for X, y in train_loader:\n",
    "            nn_outputs = net(X)\n",
    "            loss1 = criterion(nn_outputs, y) + l1alpha * net.l1reg()\n",
    "            optimizer.zero_grad()\n",
    "            loss1.backward()\n",
    "            loss.append(loss1.item())\n",
    "            optimizer.step()\n",
    "            net.positivate()\n",
    "        net.eval()\n",
    "        val_loss = []\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device)\n",
    "            nn_outputs = net(X)\n",
    "            val_loss1 = criterion(nn_outputs,y)\n",
    "            val_loss.append(val_loss1.item())\n",
    "         \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        freq = max(epochs//20,1)\n",
    "        if verbose and epoch%freq==0:\n",
    "            print('Epoch {}/{} || Loss:  Train {:.4f} | Validation {:.4f}'.format(epoch, epochs, np.mean(loss), np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/300 || Loss:  Train 1.8697 | Validation 1.9052\n",
      "Epoch 30/300 || Loss:  Train 1.4758 | Validation 1.5126\n",
      "Epoch 45/300 || Loss:  Train 1.0662 | Validation 1.1344\n",
      "Epoch 60/300 || Loss:  Train 0.9406 | Validation 1.0101\n",
      "Epoch 75/300 || Loss:  Train 0.8591 | Validation 0.9046\n",
      "Epoch 90/300 || Loss:  Train 0.7875 | Validation 0.8420\n",
      "Epoch 105/300 || Loss:  Train 0.7264 | Validation 0.7858\n",
      "Epoch 120/300 || Loss:  Train 0.6772 | Validation 0.7455\n",
      "Epoch 135/300 || Loss:  Train 0.6224 | Validation 0.6953\n",
      "Epoch 150/300 || Loss:  Train 0.5756 | Validation 0.6480\n",
      "Epoch 165/300 || Loss:  Train 0.5272 | Validation 0.6156\n",
      "Epoch 180/300 || Loss:  Train 0.4770 | Validation 0.5761\n",
      "Epoch 195/300 || Loss:  Train 0.4354 | Validation 0.5297\n",
      "Epoch 210/300 || Loss:  Train 0.3787 | Validation 0.4898\n",
      "Epoch 225/300 || Loss:  Train 0.3267 | Validation 0.4236\n",
      "Epoch 240/300 || Loss:  Train 0.2918 | Validation 0.4116\n",
      "Epoch 255/300 || Loss:  Train 0.2462 | Validation 0.3639\n",
      "Epoch 270/300 || Loss:  Train 0.2150 | Validation 0.3500\n",
      "Epoch 285/300 || Loss:  Train 0.1970 | Validation 0.3373\n",
      "Epoch 300/300 || Loss:  Train 0.1721 | Validation 0.3264\n"
     ]
    }
   ],
   "source": [
    "train_posdense(300, net, criterion, optimizer, train_loader, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9377777777777778"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(net, val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fcpos0': 0.29921875, 'fcpos1': 0.2525, 'fcpos2': 0.14}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_sparsities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1-regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net = PosDense()  \n",
    "criterion = F.cross_entropy \n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "scheduler = None\n",
    "\n",
    "train_loader = torch_data.DataLoader(train_dset, batch_size=30, shuffle=True) \n",
    "val_loader = torch_data.DataLoader(val_dset, batch_size=100, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/350 || Loss:  Train 1.7239 | Validation 1.7186\n",
      "Epoch 34/350 || Loss:  Train 1.5065 | Validation 1.5309\n",
      "Epoch 51/350 || Loss:  Train 1.3759 | Validation 1.4565\n",
      "Epoch 68/350 || Loss:  Train 1.2909 | Validation 1.3545\n",
      "Epoch 85/350 || Loss:  Train 1.2349 | Validation 1.3132\n",
      "Epoch 102/350 || Loss:  Train 1.1477 | Validation 1.2348\n",
      "Epoch 119/350 || Loss:  Train 0.9866 | Validation 1.0758\n",
      "Epoch 136/350 || Loss:  Train 0.8391 | Validation 0.9099\n",
      "Epoch 153/350 || Loss:  Train 0.7170 | Validation 0.7803\n",
      "Epoch 170/350 || Loss:  Train 0.6501 | Validation 0.7190\n",
      "Epoch 187/350 || Loss:  Train 0.5748 | Validation 0.6507\n",
      "Epoch 204/350 || Loss:  Train 0.5226 | Validation 0.6049\n",
      "Epoch 221/350 || Loss:  Train 0.4785 | Validation 0.5371\n",
      "Epoch 238/350 || Loss:  Train 0.4201 | Validation 0.5008\n",
      "Epoch 255/350 || Loss:  Train 0.3864 | Validation 0.4512\n",
      "Epoch 272/350 || Loss:  Train 0.3441 | Validation 0.4368\n",
      "Epoch 289/350 || Loss:  Train 0.3020 | Validation 0.4044\n",
      "Epoch 306/350 || Loss:  Train 0.2807 | Validation 0.3748\n",
      "Epoch 323/350 || Loss:  Train 0.2494 | Validation 0.3451\n",
      "Epoch 340/350 || Loss:  Train 0.2342 | Validation 0.3487\n"
     ]
    }
   ],
   "source": [
    "train_posdense(350, net, criterion, optimizer, train_loader, val_loader, scheduler, l1alpha=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9133333333333333"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(net, val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fcpos0': 0.3671875, 'fcpos1': 0.18625, 'fcpos2': 0.045}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_sparsities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net = FCN()  \n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "scheduler = None\n",
    "\n",
    "train_loader = torch_data.DataLoader(train_dset, batch_size=30, shuffle=True) \n",
    "val_loader = torch_data.DataLoader(val_dset, batch_size=100, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/300 || Loss:  Train 0.7658 | Validation 0.7679\n",
      "Epoch 30/300 || Loss:  Train 0.4519 | Validation 0.4594\n",
      "Epoch 45/300 || Loss:  Train 0.3258 | Validation 0.3434\n",
      "Epoch 60/300 || Loss:  Train 0.2498 | Validation 0.2646\n",
      "Epoch 75/300 || Loss:  Train 0.2031 | Validation 0.2246\n",
      "Epoch 90/300 || Loss:  Train 0.1678 | Validation 0.2048\n",
      "Epoch 105/300 || Loss:  Train 0.1447 | Validation 0.1899\n",
      "Epoch 120/300 || Loss:  Train 0.1221 | Validation 0.1846\n",
      "Epoch 135/300 || Loss:  Train 0.1081 | Validation 0.1860\n",
      "Epoch 150/300 || Loss:  Train 0.0954 | Validation 0.1707\n",
      "Epoch 165/300 || Loss:  Train 0.0861 | Validation 0.1673\n",
      "Epoch 180/300 || Loss:  Train 0.0773 | Validation 0.1723\n",
      "Epoch 195/300 || Loss:  Train 0.0687 | Validation 0.1680\n",
      "Epoch 210/300 || Loss:  Train 0.0634 | Validation 0.1768\n",
      "Epoch 225/300 || Loss:  Train 0.0576 | Validation 0.1706\n",
      "Epoch 240/300 || Loss:  Train 0.0528 | Validation 0.1751\n",
      "Epoch 255/300 || Loss:  Train 0.0482 | Validation 0.1700\n",
      "Epoch 270/300 || Loss:  Train 0.0438 | Validation 0.1794\n",
      "Epoch 285/300 || Loss:  Train 0.0409 | Validation 0.1757\n",
      "Epoch 300/300 || Loss:  Train 0.0365 | Validation 0.1793\n"
     ]
    }
   ],
   "source": [
    "train_fcn(300, net, criterion, optimizer, train_loader, val_loader, scheduler, l1alpha=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644444444444444"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(net, val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc0': 0.0, 'fc1': 0.0, 'fc2': 0.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_sparsities()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
