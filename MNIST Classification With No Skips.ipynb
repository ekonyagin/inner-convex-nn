{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as torch_data\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from poslayers.poslayers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the dataset.\n",
    "digits, targets = load_digits(return_X_y=True)\n",
    "digits = digits.astype(np.float32) / 255\n",
    "\n",
    "digits_train, digits_test, targets_train, targets_test = train_test_split(digits, targets, random_state=0)\n",
    "\n",
    "train_size = digits_train.shape[0]\n",
    "\n",
    "input_size = 8*8\n",
    "classes_n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTData(torch_data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super(MNISTData, self).__init__()\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx],self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(net, val_dset):\n",
    "    test_loader = torch_data.DataLoader(val_dset,batch_size = len(val_dset)) \n",
    "    net.eval()\n",
    "    for X,y in test_loader:\n",
    "        X = X.to(device)\n",
    "        nn_outputs = net(X).detach().numpy().argmax(axis = 1)\n",
    "    return accuracy_score(nn_outputs,y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = MNISTData(digits_train, targets_train) \n",
    "val_dset = MNISTData(digits_test, targets_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-convex FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(FCN, self).__init__()\n",
    "        self.fc0 = Dense(64, 40)\n",
    "        self.fc1 = Dense(40, 20)\n",
    "        self.fc2 = Dense(20, 10)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        h1 = F.relu(self.fc0(input_))\n",
    "        h2 = F.relu(self.fc1(h1))\n",
    "        h3 = self.fc2(h2)\n",
    "        return h3\n",
    "    \n",
    "    def get_sparsities(self):\n",
    "        return {\n",
    "                'fc0': self.fc0.get_sparsity(),\n",
    "                'fc1': self.fc1.get_sparsity(),\n",
    "                'fc2': self.fc2.get_sparsity(),\n",
    "        }\n",
    "    \n",
    "    def l1reg(self):\n",
    "        return self.fc0.l1reg() + self.fc1.l1reg() + self.fc2.l1reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net = FCN()  \n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "scheduler = None\n",
    "\n",
    "train_loader = torch_data.DataLoader(train_dset, batch_size=30, shuffle=True) \n",
    "val_loader = torch_data.DataLoader(val_dset, batch_size=100, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fcn(epochs, net, criterion, optimizer, train_loader, val_loader,scheduler=None, verbose=True, save_dir=None, l1alpha=0):\n",
    "    net.to(device)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "        loss = []\n",
    "        for X, y in train_loader:\n",
    "            nn_outputs = net(X)\n",
    "            loss1 = criterion(nn_outputs, y) + l1alpha * net.l1reg()\n",
    "            optimizer.zero_grad()\n",
    "            loss1.backward()\n",
    "            loss.append(loss1.item())\n",
    "            optimizer.step()\n",
    "        net.eval()\n",
    "        val_loss = []\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device)\n",
    "            nn_outputs = net(X)\n",
    "            val_loss1 = criterion(nn_outputs,y)\n",
    "            val_loss.append(val_loss1.item())\n",
    "         \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        freq = max(epochs//20,1)\n",
    "        if verbose and epoch%freq==0:\n",
    "            print('Epoch {}/{} || Loss:  Train {:.4f} | Validation {:.4f}'.format(epoch, epochs, np.mean(loss), np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 || Loss:  Train 1.3953 | Validation 1.3722\n",
      "Epoch 20/200 || Loss:  Train 0.7349 | Validation 0.7527\n",
      "Epoch 30/200 || Loss:  Train 0.4920 | Validation 0.5178\n",
      "Epoch 40/200 || Loss:  Train 0.3856 | Validation 0.4194\n",
      "Epoch 50/200 || Loss:  Train 0.3188 | Validation 0.3567\n",
      "Epoch 60/200 || Loss:  Train 0.2747 | Validation 0.3101\n",
      "Epoch 70/200 || Loss:  Train 0.2418 | Validation 0.2793\n",
      "Epoch 80/200 || Loss:  Train 0.2168 | Validation 0.2630\n",
      "Epoch 90/200 || Loss:  Train 0.1982 | Validation 0.2468\n",
      "Epoch 100/200 || Loss:  Train 0.1807 | Validation 0.2402\n",
      "Epoch 110/200 || Loss:  Train 0.1671 | Validation 0.2274\n",
      "Epoch 120/200 || Loss:  Train 0.1580 | Validation 0.2220\n",
      "Epoch 130/200 || Loss:  Train 0.1493 | Validation 0.2193\n",
      "Epoch 140/200 || Loss:  Train 0.1406 | Validation 0.2191\n",
      "Epoch 150/200 || Loss:  Train 0.1336 | Validation 0.2184\n",
      "Epoch 160/200 || Loss:  Train 0.1261 | Validation 0.2175\n",
      "Epoch 170/200 || Loss:  Train 0.1208 | Validation 0.2154\n",
      "Epoch 180/200 || Loss:  Train 0.1153 | Validation 0.2130\n",
      "Epoch 190/200 || Loss:  Train 0.1097 | Validation 0.2161\n",
      "Epoch 200/200 || Loss:  Train 0.1050 | Validation 0.2196\n"
     ]
    }
   ],
   "source": [
    "train_fcn(200, net, criterion, optimizer, train_loader, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9377777777777778"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(net, val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc0': 0.0, 'fc1': 0.0, 'fc2': 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_sparsities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POSDENSE = CNN WITHOUT ADDITIONAL LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ICNN, self).__init__()\n",
    "        self.fcpos0 = PosDense(64, 40)\n",
    "        self.fcpos1 = PosDense(40, 20)\n",
    "        self.fcpos2 = PosDense(20, 10)\n",
    "        self.fc = Dense(10, 10)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        h1 = F.relu(self.fcpos0(input_))\n",
    "        h2 = F.relu(self.fcpos1(h1))\n",
    "        h3 = F.relu(self.fcpos2(h2))\n",
    "        h4 = self.fc(h3)\n",
    "        return h4\n",
    "    \n",
    "    def positivate(self):\n",
    "        self.fcpos0.positivate()\n",
    "        self.fcpos1.positivate()\n",
    "        self.fcpos2.positivate()\n",
    "    \n",
    "    def get_sparsities(self):\n",
    "        get_sparsity = lambda layer : (layer.weight.data == 0).sum().item() / (layer.weight.data.shape[0] * layer.weight.data.shape[1])\n",
    "        return {\n",
    "                'fcpos0': get_sparsity(self.fcpos0),\n",
    "                'fcpos1': get_sparsity(self.fcpos1),\n",
    "                'fcpos2': get_sparsity(self.fcpos2),\n",
    "                'fc' : get_sparsity(self.fc)\n",
    "        }\n",
    "    \n",
    "    def l1reg(self):\n",
    "        return self.fcpos0.l1reg() + self.fcpos1.l1reg() + self.fcpos2.l1reg() + self.fc.l1reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net = ICNN()  \n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "scheduler = None\n",
    "\n",
    "train_loader = torch_data.DataLoader(train_dset, batch_size=30, shuffle=True) \n",
    "val_loader = torch_data.DataLoader(val_dset, batch_size=100, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_icnn(epochs, net, criterion, optimizer, train_loader, val_loader,scheduler=None, verbose=True, save_dir=None, l1alpha=0):\n",
    "    net.to(device)\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.train()\n",
    "        loss = []\n",
    "        for X, y in train_loader:\n",
    "            nn_outputs = net(X)\n",
    "            loss1 = criterion(nn_outputs, y) + l1alpha * net.l1reg()\n",
    "            optimizer.zero_grad()\n",
    "            loss1.backward()\n",
    "            loss.append(loss1.item())\n",
    "            optimizer.step()\n",
    "            net.positivate()\n",
    "        net.eval()\n",
    "        val_loss = []\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(device)\n",
    "            nn_outputs = net(X)\n",
    "            val_loss1 = criterion(nn_outputs,y)\n",
    "            val_loss.append(val_loss1.item())\n",
    "         \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        freq = max(epochs//20,1)\n",
    "        if verbose and epoch%freq==0:\n",
    "            print('Epoch {}/{} || Loss:  Train {:.4f} | Validation {:.4f}'.format(epoch, epochs, np.mean(loss), np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/300 || Loss:  Train 1.9431 | Validation 1.9599\n",
      "Epoch 30/300 || Loss:  Train 1.7851 | Validation 1.8151\n",
      "Epoch 45/300 || Loss:  Train 1.5791 | Validation 1.6146\n",
      "Epoch 60/300 || Loss:  Train 1.2900 | Validation 1.3047\n",
      "Epoch 75/300 || Loss:  Train 1.0000 | Validation 1.0520\n",
      "Epoch 90/300 || Loss:  Train 0.7966 | Validation 0.8885\n",
      "Epoch 105/300 || Loss:  Train 0.5972 | Validation 0.7218\n",
      "Epoch 120/300 || Loss:  Train 0.5055 | Validation 0.6492\n",
      "Epoch 135/300 || Loss:  Train 0.4568 | Validation 0.5881\n",
      "Epoch 150/300 || Loss:  Train 0.4142 | Validation 0.5609\n",
      "Epoch 165/300 || Loss:  Train 0.3866 | Validation 0.5354\n",
      "Epoch 180/300 || Loss:  Train 0.3500 | Validation 0.5169\n",
      "Epoch 195/300 || Loss:  Train 0.3197 | Validation 0.5063\n",
      "Epoch 210/300 || Loss:  Train 0.2905 | Validation 0.4770\n",
      "Epoch 225/300 || Loss:  Train 0.2715 | Validation 0.4805\n",
      "Epoch 240/300 || Loss:  Train 0.2474 | Validation 0.4487\n",
      "Epoch 255/300 || Loss:  Train 0.2277 | Validation 0.4421\n",
      "Epoch 270/300 || Loss:  Train 0.2154 | Validation 0.4218\n",
      "Epoch 285/300 || Loss:  Train 0.1853 | Validation 0.4214\n",
      "Epoch 300/300 || Loss:  Train 0.1738 | Validation 0.4172\n"
     ]
    }
   ],
   "source": [
    "train_icnn(300, net, criterion, optimizer, train_loader, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(net, val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fcpos0': 0.31328125, 'fcpos1': 0.18, 'fcpos2': 0.24, 'fc': 0.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_sparsities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1-regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net = ICNN()  \n",
    "criterion = F.cross_entropy \n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "scheduler = None\n",
    "\n",
    "train_loader = torch_data.DataLoader(train_dset, batch_size=30, shuffle=True) \n",
    "val_loader = torch_data.DataLoader(val_dset, batch_size=100, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/350 || Loss:  Train 1.5012 | Validation 1.5382\n",
      "Epoch 34/350 || Loss:  Train 1.1631 | Validation 1.1798\n",
      "Epoch 51/350 || Loss:  Train 0.9267 | Validation 0.9423\n",
      "Epoch 68/350 || Loss:  Train 0.8362 | Validation 0.8901\n",
      "Epoch 85/350 || Loss:  Train 0.7647 | Validation 0.7593\n",
      "Epoch 102/350 || Loss:  Train 0.7192 | Validation 0.7279\n",
      "Epoch 119/350 || Loss:  Train 0.6949 | Validation 0.6816\n",
      "Epoch 136/350 || Loss:  Train 0.6695 | Validation 0.6848\n",
      "Epoch 153/350 || Loss:  Train 0.6397 | Validation 0.6734\n",
      "Epoch 170/350 || Loss:  Train 0.6124 | Validation 0.6449\n",
      "Epoch 187/350 || Loss:  Train 0.5960 | Validation 0.6226\n",
      "Epoch 204/350 || Loss:  Train 0.5760 | Validation 0.6136\n",
      "Epoch 221/350 || Loss:  Train 0.5618 | Validation 0.6020\n",
      "Epoch 238/350 || Loss:  Train 0.5347 | Validation 0.5716\n",
      "Epoch 255/350 || Loss:  Train 0.5006 | Validation 0.5428\n",
      "Epoch 272/350 || Loss:  Train 0.4660 | Validation 0.5073\n",
      "Epoch 289/350 || Loss:  Train 0.4157 | Validation 0.4775\n",
      "Epoch 306/350 || Loss:  Train 0.3723 | Validation 0.4282\n",
      "Epoch 323/350 || Loss:  Train 0.3480 | Validation 0.4173\n",
      "Epoch 340/350 || Loss:  Train 0.3140 | Validation 0.3641\n"
     ]
    }
   ],
   "source": [
    "train_icnn(350, net, criterion, optimizer, train_loader, val_loader, scheduler, l1alpha=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8955555555555555"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(net, val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fcpos0': 0.33046875, 'fcpos1': 0.37875, 'fcpos2': 0.215, 'fc': 0.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_sparsities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "net = FCN()  \n",
    "criterion = F.cross_entropy\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "scheduler = None\n",
    "\n",
    "train_loader = torch_data.DataLoader(train_dset, batch_size=30, shuffle=True) \n",
    "val_loader = torch_data.DataLoader(val_dset, batch_size=100, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/300 || Loss:  Train 0.7673 | Validation 0.7889\n",
      "Epoch 30/300 || Loss:  Train 0.4360 | Validation 0.4594\n",
      "Epoch 45/300 || Loss:  Train 0.3227 | Validation 0.3555\n",
      "Epoch 60/300 || Loss:  Train 0.2584 | Validation 0.3157\n",
      "Epoch 75/300 || Loss:  Train 0.2135 | Validation 0.2613\n",
      "Epoch 90/300 || Loss:  Train 0.1820 | Validation 0.2569\n",
      "Epoch 105/300 || Loss:  Train 0.1588 | Validation 0.2351\n",
      "Epoch 120/300 || Loss:  Train 0.1427 | Validation 0.2264\n",
      "Epoch 135/300 || Loss:  Train 0.1252 | Validation 0.2179\n",
      "Epoch 150/300 || Loss:  Train 0.1125 | Validation 0.2329\n",
      "Epoch 165/300 || Loss:  Train 0.1075 | Validation 0.2210\n",
      "Epoch 180/300 || Loss:  Train 0.0922 | Validation 0.2213\n",
      "Epoch 195/300 || Loss:  Train 0.0848 | Validation 0.2197\n",
      "Epoch 210/300 || Loss:  Train 0.0762 | Validation 0.2288\n",
      "Epoch 225/300 || Loss:  Train 0.0699 | Validation 0.2216\n",
      "Epoch 240/300 || Loss:  Train 0.0656 | Validation 0.2264\n"
     ]
    }
   ],
   "source": [
    "train_fcn(300, net, criterion, optimizer, train_loader, val_loader, scheduler, l1alpha=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(net, val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.get_sparsities()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
